{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "9de7f0a4-8788-464f-8e30-65a0b65e3991",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "\n",
    "load_dotenv()  # .env 파일 로드\n",
    "hf_token = os.getenv('HF_TOKEN')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "afc37ce3",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "586f38d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repo\\finetune_sogm123\\sogm123_venv\\Lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "459e6c86-70cf-4627-b001-ae5d8d0fa448",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "`low_cpu_mem_usage` was None, now default to True since model is quantized.\n",
      "Loading checkpoint shards: 100%|██████████| 4/4 [00:06<00:00,  1.61s/it]\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer, AutoModelForCausalLM, BitsAndBytesConfig\n",
    "\n",
    "model_id = \"meta-llama/Meta-Llama-3-8B-Instruct\"\n",
    "\n",
    "# BitsAndBytesConfig 설정\n",
    "quantization_config = BitsAndBytesConfig(\n",
    "    load_in_4bit=True,  # 8비트 대신 4비트 사용\n",
    "    bnb_4bit_compute_dtype=torch.float16,\n",
    "    bnb_4bit_quant_type=\"nf4\",\n",
    "    bnb_4bit_use_double_quant=True\n",
    ")\n",
    "\n",
    "tokenizer = AutoTokenizer.from_pretrained(model_id)\n",
    "model = AutoModelForCausalLM.from_pretrained(\n",
    "    model_id,\n",
    "    quantization_config=quantization_config,\n",
    "    #device_map=\"cuda\",\n",
    "    trust_remote_code=True\n",
    ")\n",
    "\n",
    "from peft import prepare_model_for_kbit_training\n",
    "model = prepare_model_for_kbit_training(model)\n",
    "\n",
    "model.gradient_checkpointing_enable()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5c8a0ebf-f827-459f-8cb5-c87407c443cc",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "LlamaForCausalLM(\n",
       "  (model): LlamaModel(\n",
       "    (embed_tokens): Embedding(128256, 4096)\n",
       "    (layers): ModuleList(\n",
       "      (0-31): 32 x LlamaDecoderLayer(\n",
       "        (self_attn): LlamaAttention(\n",
       "          (q_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "          (k_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (v_proj): Linear4bit(in_features=4096, out_features=1024, bias=False)\n",
       "          (o_proj): Linear4bit(in_features=4096, out_features=4096, bias=False)\n",
       "        )\n",
       "        (mlp): LlamaMLP(\n",
       "          (gate_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (up_proj): Linear4bit(in_features=4096, out_features=14336, bias=False)\n",
       "          (down_proj): Linear4bit(in_features=14336, out_features=4096, bias=False)\n",
       "          (act_fn): SiLU()\n",
       "        )\n",
       "        (input_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "        (post_attention_layernorm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "      )\n",
       "    )\n",
       "    (norm): LlamaRMSNorm((4096,), eps=1e-05)\n",
       "    (rotary_emb): LlamaRotaryEmbedding()\n",
       "  )\n",
       "  (lm_head): Linear(in_features=4096, out_features=128256, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5a0bca38-8fc2-42bb-adb0-166cc8fd22ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "for param in model.parameters():\n",
    "  param.requires_grad = False  # freeze the model - train adapters later\n",
    "  if param.ndim == 1:\n",
    "    param.data = param.data.to(torch.float32)\n",
    "\n",
    "model.gradient_checkpointing_enable()  # reduce number of stored activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "278857ef-c9ee-4a99-95e6-feadb438608f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_trainable_parameters(model):\n",
    "    \"\"\"\n",
    "    Prints the number of trainable parameters in the model.\n",
    "    \"\"\"\n",
    "    trainable_params = 0\n",
    "    all_param = 0\n",
    "    for _, param in model.named_parameters():\n",
    "        all_param += param.numel()\n",
    "        if param.requires_grad:\n",
    "            trainable_params += param.numel()\n",
    "    print(\n",
    "        f\"trainable params: {trainable_params:,} || all params: {all_param:,} || trainable : {100 * trainable_params / all_param}%\"\n",
    "    )\n",
    "    return trainable_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d91bf9e1-549b-451e-8184-758ff23a26d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 0 || all params: 4,540,600,320 || trainable : 0.0%\n"
     ]
    }
   ],
   "source": [
    "ori_p = print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "edfe2585-c5a3-4225-adf5-4f198838a170",
   "metadata": {},
   "outputs": [],
   "source": [
    "from peft import LoraConfig, get_peft_model\n",
    "\n",
    "config = LoraConfig(\n",
    "    r=16, #attention heads\n",
    "    lora_alpha=32, #alpha scaling\n",
    "    lora_dropout=0.05,\n",
    "    bias=\"none\",\n",
    "    task_type=\"CAUSAL_LM\" # set this for CAUSAL LANGUAGE MODELS (like Bloom, LLaMA) or SEQ TO SEQ (like FLAN, T5)\n",
    ")\n",
    "\n",
    "model = get_peft_model(model, config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "066e6c7b-1c9d-489d-8124-99eaebcbe621",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trainable params: 6,815,744 || all params: 4,547,416,064 || trainable : 0.14988168894325302%\n"
     ]
    }
   ],
   "source": [
    "peft_p = print_trainable_parameters(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "4aae5c2c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting datasets\n",
      "  Downloading datasets-3.3.0-py3-none-any.whl.metadata (19 kB)\n",
      "Requirement already satisfied: filelock in .\\sogm123_venv\\lib\\site-packages (from datasets) (3.17.0)\n",
      "Requirement already satisfied: numpy>=1.17 in .\\sogm123_venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Collecting pyarrow>=15.0.0 (from datasets)\n",
      "  Downloading pyarrow-19.0.0-cp313-cp313-win_amd64.whl.metadata (3.4 kB)\n",
      "Collecting dill<0.3.9,>=0.3.0 (from datasets)\n",
      "  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n",
      "Requirement already satisfied: pandas in .\\sogm123_venv\\lib\\site-packages (from datasets) (2.2.3)\n",
      "Requirement already satisfied: requests>=2.32.2 in .\\sogm123_venv\\lib\\site-packages (from datasets) (2.32.3)\n",
      "Requirement already satisfied: tqdm>=4.66.3 in .\\sogm123_venv\\lib\\site-packages (from datasets) (4.67.1)\n",
      "Collecting xxhash (from datasets)\n",
      "  Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl.metadata (13 kB)\n",
      "Collecting multiprocess<0.70.17 (from datasets)\n",
      "  Downloading multiprocess-0.70.16-py312-none-any.whl.metadata (7.2 kB)\n",
      "Collecting fsspec<=2024.12.0,>=2023.1.0 (from fsspec[http]<=2024.12.0,>=2023.1.0->datasets)\n",
      "  Downloading fsspec-2024.12.0-py3-none-any.whl.metadata (11 kB)\n",
      "Collecting aiohttp (from datasets)\n",
      "  Downloading aiohttp-3.11.12-cp313-cp313-win_amd64.whl.metadata (8.0 kB)\n",
      "Requirement already satisfied: huggingface-hub>=0.24.0 in .\\sogm123_venv\\lib\\site-packages (from datasets) (0.28.1)\n",
      "Requirement already satisfied: packaging in .\\sogm123_venv\\lib\\site-packages (from datasets) (24.2)\n",
      "Requirement already satisfied: pyyaml>=5.1 in .\\sogm123_venv\\lib\\site-packages (from datasets) (6.0.2)\n",
      "Collecting aiohappyeyeballs>=2.3.0 (from aiohttp->datasets)\n",
      "  Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl.metadata (5.9 kB)\n",
      "Collecting aiosignal>=1.1.2 (from aiohttp->datasets)\n",
      "  Downloading aiosignal-1.3.2-py2.py3-none-any.whl.metadata (3.8 kB)\n",
      "Collecting attrs>=17.3.0 (from aiohttp->datasets)\n",
      "  Using cached attrs-25.1.0-py3-none-any.whl.metadata (10 kB)\n",
      "Collecting frozenlist>=1.1.1 (from aiohttp->datasets)\n",
      "  Downloading frozenlist-1.5.0-cp313-cp313-win_amd64.whl.metadata (14 kB)\n",
      "Collecting multidict<7.0,>=4.5 (from aiohttp->datasets)\n",
      "  Downloading multidict-6.1.0-cp313-cp313-win_amd64.whl.metadata (5.1 kB)\n",
      "Collecting propcache>=0.2.0 (from aiohttp->datasets)\n",
      "  Downloading propcache-0.2.1-cp313-cp313-win_amd64.whl.metadata (9.5 kB)\n",
      "Collecting yarl<2.0,>=1.17.0 (from aiohttp->datasets)\n",
      "  Downloading yarl-1.18.3-cp313-cp313-win_amd64.whl.metadata (71 kB)\n",
      "Requirement already satisfied: typing-extensions>=3.7.4.3 in .\\sogm123_venv\\lib\\site-packages (from huggingface-hub>=0.24.0->datasets) (4.12.2)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in .\\sogm123_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.4.1)\n",
      "Requirement already satisfied: idna<4,>=2.5 in .\\sogm123_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (3.10)\n",
      "Requirement already satisfied: urllib3<3,>=1.21.1 in .\\sogm123_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2.3.0)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in .\\sogm123_venv\\lib\\site-packages (from requests>=2.32.2->datasets) (2025.1.31)\n",
      "Requirement already satisfied: colorama in .\\sogm123_venv\\lib\\site-packages (from tqdm>=4.66.3->datasets) (0.4.6)\n",
      "Requirement already satisfied: python-dateutil>=2.8.2 in .\\sogm123_venv\\lib\\site-packages (from pandas->datasets) (2.9.0.post0)\n",
      "Requirement already satisfied: pytz>=2020.1 in .\\sogm123_venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: tzdata>=2022.7 in .\\sogm123_venv\\lib\\site-packages (from pandas->datasets) (2025.1)\n",
      "Requirement already satisfied: six>=1.5 in .\\sogm123_venv\\lib\\site-packages (from python-dateutil>=2.8.2->pandas->datasets) (1.17.0)\n",
      "Downloading datasets-3.3.0-py3-none-any.whl (484 kB)\n",
      "Downloading dill-0.3.8-py3-none-any.whl (116 kB)\n",
      "Downloading fsspec-2024.12.0-py3-none-any.whl (183 kB)\n",
      "Downloading aiohttp-3.11.12-cp313-cp313-win_amd64.whl (436 kB)\n",
      "Downloading multiprocess-0.70.16-py312-none-any.whl (146 kB)\n",
      "Downloading pyarrow-19.0.0-cp313-cp313-win_amd64.whl (25.2 MB)\n",
      "   ---------------------------------------- 0.0/25.2 MB ? eta -:--:--\n",
      "   ---- ----------------------------------- 2.6/25.2 MB 13.1 MB/s eta 0:00:02\n",
      "   --------- ------------------------------ 6.3/25.2 MB 15.6 MB/s eta 0:00:02\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 17.4 MB/s eta 0:00:01\n",
      "   ---------------- ----------------------- 10.5/25.2 MB 17.4 MB/s eta 0:00:01\n",
      "   -------------------- ------------------- 13.1/25.2 MB 12.5 MB/s eta 0:00:01\n",
      "   ------------------------------------ --- 23.3/25.2 MB 18.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 25.2/25.2 MB 18.8 MB/s eta 0:00:00\n",
      "Downloading xxhash-3.5.0-cp313-cp313-win_amd64.whl (30 kB)\n",
      "Downloading aiohappyeyeballs-2.4.6-py3-none-any.whl (14 kB)\n",
      "Downloading aiosignal-1.3.2-py2.py3-none-any.whl (7.6 kB)\n",
      "Using cached attrs-25.1.0-py3-none-any.whl (63 kB)\n",
      "Downloading frozenlist-1.5.0-cp313-cp313-win_amd64.whl (51 kB)\n",
      "Downloading multidict-6.1.0-cp313-cp313-win_amd64.whl (28 kB)\n",
      "Downloading propcache-0.2.1-cp313-cp313-win_amd64.whl (43 kB)\n",
      "Downloading yarl-1.18.3-cp313-cp313-win_amd64.whl (315 kB)\n",
      "Installing collected packages: xxhash, pyarrow, propcache, multidict, fsspec, frozenlist, dill, attrs, aiohappyeyeballs, yarl, multiprocess, aiosignal, aiohttp, datasets\n",
      "  Attempting uninstall: fsspec\n",
      "    Found existing installation: fsspec 2025.2.0\n",
      "    Uninstalling fsspec-2025.2.0:\n",
      "      Successfully uninstalled fsspec-2025.2.0\n",
      "Successfully installed aiohappyeyeballs-2.4.6 aiohttp-3.11.12 aiosignal-1.3.2 attrs-25.1.0 datasets-3.3.0 dill-0.3.8 frozenlist-1.5.0 fsspec-2024.12.0 multidict-6.1.0 multiprocess-0.70.16 propcache-0.2.1 pyarrow-19.0.0 xxhash-3.5.0 yarl-1.18.3\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.3.1 -> 25.0.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "!pip install datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "d16e4797-cbde-46e7-9a4d-6b7789e9a774",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\repo\\finetune_sogm123\\sogm123_venv\\Lib\\site-packages\\huggingface_hub\\file_download.py:140: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\user\\.cache\\huggingface\\hub\\datasets--kimjaewon--baemin_sft_data. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
      "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
      "  warnings.warn(message)\n",
      "Generating train split: 100%|██████████| 1452/1452 [00:00<00:00, 156792.37 examples/s]\n"
     ]
    }
   ],
   "source": [
    "import transformers\n",
    "from datasets import load_dataset\n",
    "dataset = load_dataset(\"kimjaewon/baemin_sft_data\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "739bb674-2e89-4815-bb67-29343b149521",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'positive_document_list', 'negative_document_list', 'answer'],\n",
       "        num_rows: 1452\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "1dd7b5c4-3fdf-4355-a3c9-27adcc2fdd30",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer'],\n",
       "        num_rows: 1452\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset = dataset.remove_columns(['positive_document_list', 'negative_document_list'])\n",
    "peft_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "54440e3c-774a-4913-822a-a14e50d3d284",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['정산이 이뤄지는 시점은 언제인가요?',\n",
       " '주문 차단을 하고 싶다면 어떻게 해야 하나요?',\n",
       " '탈퇴 회원의 댓글 작성자 본인 여부를 확인할 수 있는 방법은 없나요?',\n",
       " '네이버 플레이스에 가게 정보를 제공하면 연동 여부 반영까지 얼마나 걸리나요?',\n",
       " '어떤 음식은 배달의민족을 통해 판매할 수 없나요?',\n",
       " '어떤 경우에 고객센터로 문의해야 하나요?',\n",
       " '울트라콜과 오픈리스트 상품의 배달팁은 어떻게 구분되나요?',\n",
       " '배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요?']"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset['train']['question'][:8]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c9d401d2-5209-4e41-8f19-3df87543c4e6",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1452/1452 [00:00<00:00, 31617.82 examples/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "['어떤 경우에 고객센터로 문의해야 하나요? ===> 고객의 개인정보와 관련된 문의와 함께 배달의 불만사항, 결제문제 등 배달 서비스와 관련된 문제가 발생한 경우, 그리고 배달 관련 추가 연락이 필요한 경우에는 고객센터로 문의해주시면 최대한 도움을 드리겠습니다.',\n",
       " '울트라콜과 오픈리스트 상품의 배달팁은 어떻게 구분되나요? ===> 울트라콜과 오픈리스트 상품의 배달팁은 용도에 따라 기본 배달팁과 할증 배달팁으로 나누어집니다. 기본 배달팁은 주문금액에 따른 최대 3개의 배달팁 설정이 가능하며, 할증 배달팁은 지역, 시간대, 공휴일 등을 위해 별도로 설정이 가능합니다.']"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def merge_cols(example):\n",
    "    example[\"prediction\"] = example[\"question\"] + \" ===> \" + example[\"answer\"]\n",
    "    return example\n",
    "\n",
    "peft_dataset['train'] = peft_dataset['train'].map(merge_cols) # <-- 모든 문장에 대해 처리해 줍니다.\n",
    "peft_dataset['train'][\"prediction\"][5:7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "78a5e1fd-3594-4b40-83bd-1b3df1ef4cea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'question': '배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요?',\n",
       " 'answer': '아니요, 배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 불가능합니다. 배달지역 조회만 가능하며, 배달지역 수정이 필요한 경우 고객센터를 통해 문의하셔야 합니다.',\n",
       " 'prediction': '배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요? ===> 아니요, 배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 불가능합니다. 배달지역 조회만 가능하며, 배달지역 수정이 필요한 경우 고객센터를 통해 문의하셔야 합니다.'}"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset['train'][7]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "bf10b97c-a022-4d4f-82bd-7ba66fd938e2",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Map: 100%|██████████| 1452/1452 [00:00<00:00, 17370.00 examples/s]\n"
     ]
    }
   ],
   "source": [
    "peft_dataset = peft_dataset.map(\n",
    "                        lambda x: tokenizer(x['prediction']),\n",
    "                        batched=True\n",
    "                     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "e2766dc6-8a56-4282-be14-cd93ca59e9b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DatasetDict({\n",
       "    train: Dataset({\n",
       "        features: ['question', 'answer', 'prediction', 'input_ids', 'attention_mask'],\n",
       "        num_rows: 1452\n",
       "    })\n",
       "})"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peft_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "1c9a0e36-1f59-4c84-a00e-a4c866463c7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_args = transformers.TrainingArguments(\n",
    "    output_dir='outputs',\n",
    "    \n",
    "    # 학습 관련 설정\n",
    "    per_device_train_batch_size=1,      # 배치 사이즈 1로 설정\n",
    "    gradient_accumulation_steps=8,       # 8번 누적 (효과적 배치 사이즈 = 8)\n",
    "    learning_rate=2e-4,\n",
    "    \n",
    "    # 최적화 관련 설정\n",
    "    fp16=True,                          # fp16 사용\n",
    "    optim=\"adamw_torch_fused\",          # 메모리 효율적인 옵티마이저\n",
    "    \n",
    "    # 스케줄링\n",
    "    warmup_steps=100,\n",
    "    max_steps=20,\n",
    "    \n",
    "    # 로깅\n",
    "    logging_steps=20,\n",
    ")\n",
    "\n",
    "tokenizer.pad_token = tokenizer.eos_token \n",
    "model=model.to(\"cuda\")\n",
    "\n",
    "trainer = transformers.Trainer(\n",
    "    model=model,\n",
    "    train_dataset=peft_dataset['train'],\n",
    "    args=train_args,\n",
    "    data_collator=transformers.DataCollatorForLanguageModeling(tokenizer, mlm=False)\n",
    ")\n",
    "model.config.use_cache = False  # silence the warnings. Please re-enable for inference!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "aa4e5f33",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sun Feb 16 03:09:12 2025       \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 531.18                 Driver Version: 531.18       CUDA Version: 12.1     |\n",
      "|-----------------------------------------+----------------------+----------------------+\n",
      "| GPU  Name                      TCC/WDDM | Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf            Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                                         |                      |               MIG M. |\n",
      "|=========================================+======================+======================|\n",
      "|   0  NVIDIA GeForce RTX 3080       WDDM | 00000000:01:00.0  On |                  N/A |\n",
      "| 31%   48C    P5               46W / 370W|   9718MiB / 10240MiB |     13%      Default |\n",
      "|                                         |                      |                  N/A |\n",
      "+-----------------------------------------+----------------------+----------------------+\n",
      "                                                                                         \n",
      "+---------------------------------------------------------------------------------------+\n",
      "| Processes:                                                                            |\n",
      "|  GPU   GI   CI        PID   Type   Process name                            GPU Memory |\n",
      "|        ID   ID                                                             Usage      |\n",
      "|=======================================================================================|\n",
      "|    0   N/A  N/A      1680    C+G   ...__8wekyb3d8bbwe\\Microsoft.Notes.exe    N/A      |\n",
      "|    0   N/A  N/A      7624    C+G   ...on\\133.0.3065.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A      9024    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A      9284    C+G   C:\\Windows\\explorer.exe                   N/A      |\n",
      "|    0   N/A  N/A     10868    C+G   ...nt.CBS_cw5n1h2txyewy\\SearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     10892    C+G   ...2txyewy\\StartMenuExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     12524    C+G   ...oogle\\Chrome\\Application\\chrome.exe    N/A      |\n",
      "|    0   N/A  N/A     13068    C+G   ...ekyb3d8bbwe\\PhoneExperienceHost.exe    N/A      |\n",
      "|    0   N/A  N/A     14660    C+G   ...CBS_cw5n1h2txyewy\\TextInputHost.exe    N/A      |\n",
      "|    0   N/A  N/A     15056    C+G   ...les\\Microsoft OneDrive\\OneDrive.exe    N/A      |\n",
      "|    0   N/A  N/A     15596    C+G   C:\\Nexon\\NexonPlug\\NexonPlug.exe          N/A      |\n",
      "|    0   N/A  N/A     17356    C+G   ...crosoft\\Edge\\Application\\msedge.exe    N/A      |\n",
      "|    0   N/A  N/A     18260    C+G   ...al\\Discord\\app-1.0.9182\\Discord.exe    N/A      |\n",
      "|    0   N/A  N/A     19548    C+G   ...on\\133.0.3065.69\\msedgewebview2.exe    N/A      |\n",
      "|    0   N/A  N/A     21140    C+G   ...__8wekyb3d8bbwe\\WindowsTerminal.exe    N/A      |\n",
      "|    0   N/A  N/A     25204    C+G   ...siveControlPanel\\SystemSettings.exe    N/A      |\n",
      "|    0   N/A  N/A     25456    C+G   ....CBS_cw5n1h2txyewy\\FESearchHost.exe    N/A      |\n",
      "|    0   N/A  N/A     29172      C   ...rograms\\Python\\Python313\\python.exe    N/A      |\n",
      "|    0   N/A  N/A     29572    C+G   ...Programs\\Microsoft VS Code\\Code.exe    N/A      |\n",
      "+---------------------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "618957a1-0c32-4b32-ae3b-fc3d90bc1cec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='20' max='20' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [20/20 01:53, Epoch 0/1]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Step</th>\n",
       "      <th>Training Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>2.839500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=20, training_loss=2.8395259857177733, metrics={'train_runtime': 118.7224, 'train_samples_per_second': 1.348, 'train_steps_per_second': 0.168, 'total_flos': 565408701112320.0, 'train_loss': 2.8395259857177733, 'epoch': 0.11019283746556474})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "207b8b3f-a1c5-42a9-9e09-6950ffaca4a5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "peft_ai :  배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요? ===> 한국어로 대답하세요.\n",
      "\n",
      "배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 불가능합니다. 배민라이더스는 배달지역 수정이 불가능한 기본적인 서비스입니다. 배민셀프서비스는 배민라이더스 외의 서비스입니다. 배민셀프서비스를 사용하여 배달지역을 수정하려면 배민셀프서비스를 사용하여야 합니다.\n",
      "\n",
      "Translation:\n",
      "\n",
      "Can I modify the delivery area through Minim Self-Service as a Minim Rider? ===> Answer in Korean.\n",
      "\n",
      "As a Minim Rider, you cannot modify the delivery area through Minim Self-Service. The\n"
     ]
    }
   ],
   "source": [
    "# 새로운 amp 문법 사용 및 디바이스 이동\n",
    "user_message = tokenizer(\"배민라이더스는 배민셀프서비스를 통한 배달지역 수정이 가능한가요? ===> 한국어로 대답하세요\", return_tensors='pt')\n",
    "user_message = {k: v.to('cuda') for k, v in user_message.items()}  # 모든 입력을 GPU로\n",
    "\n",
    "with torch.amp.autocast('cuda'):  # 새로운 문법\n",
    "    output_tokens = model.generate(\n",
    "        **user_message,\n",
    "        pad_token_id=tokenizer.eos_token_id,\n",
    "        max_new_tokens=128,\n",
    "        do_sample=True,          # 다양한 응답을 위해\n",
    "        temperature=0.7,         # 창의성 조절\n",
    "        top_p=0.9               # nucleus sampling\n",
    "    )\n",
    "\n",
    "print('peft_ai : ', tokenizer.decode(output_tokens[0], skip_special_tokens=True))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "4ece613c-4b6f-4d2c-b70b-9e087d42509f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/freenak/miniconda3/envs/llm_env/lib/python3.8/site-packages/huggingface_hub/file_download.py:1132: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "model_path = 'llama3_peft'  # it will be directory\n",
    "model.save_pretrained(model_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "560f6b50-69cd-4756-9d3b-64132c1596fc",
   "metadata": {},
   "outputs": [],
   "source": [
    "lora_config = LoraConfig.from_pretrained(model_path)\n",
    "model = get_peft_model(model, lora_config)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "sogm123_venv",
   "language": "python",
   "name": "sogm123_venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
